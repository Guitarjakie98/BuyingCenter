{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8f54e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning / Stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ce689b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Found 7 CSV files:\n",
      "  - 251020_ENT_2900__activity (3).csv\n",
      "  - 251020_ENT_2900__activity (2).csv\n",
      "  - 251020_ENT_2900__activity (5).csv\n",
      "  - 251020_ENT_2900__activity.csv\n",
      "  - 251020_ENT_2900__activity (4).csv\n",
      "  - 251020_ENT_2900__activity (7).csv\n",
      "  - 251020_ENT_2900__activity (1).csv\n",
      "\n",
      "üìñ Loading: 251020_ENT_2900__activity (3).csv\n",
      "   ‚úÖ Loaded 309,673 rows\n",
      "\n",
      "üìñ Loading: 251020_ENT_2900__activity (2).csv\n",
      "   ‚úÖ Loaded 277,658 rows\n",
      "\n",
      "üìñ Loading: 251020_ENT_2900__activity (5).csv\n",
      "   ‚úÖ Loaded 387,102 rows\n",
      "\n",
      "üìñ Loading: 251020_ENT_2900__activity.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9p/ktbdntkn6p388k364f6v6n500000gn/T/ipykernel_61675/3633190304.py:22: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Loaded 319,882 rows\n",
      "\n",
      "üìñ Loading: 251020_ENT_2900__activity (4).csv\n",
      "   ‚úÖ Loaded 267,546 rows\n",
      "\n",
      "üìñ Loading: 251020_ENT_2900__activity (7).csv\n",
      "   ‚úÖ Loaded 258,656 rows\n",
      "\n",
      "üìñ Loading: 251020_ENT_2900__activity (1).csv\n",
      "   ‚úÖ Loaded 156,881 rows\n",
      "\n",
      "üéâ COMBINATION COMPLETE!\n",
      "   Files combined: 7\n",
      "   Total rows: 1,977,398\n",
      "   Total columns: 19\n",
      "\n",
      "üìä Rows per file:\n",
      "   251020_ENT_2900__activity (5).csv: 387,102 rows\n",
      "   251020_ENT_2900__activity.csv: 319,882 rows\n",
      "   251020_ENT_2900__activity (3).csv: 309,673 rows\n",
      "   251020_ENT_2900__activity (2).csv: 277,658 rows\n",
      "   251020_ENT_2900__activity (4).csv: 267,546 rows\n",
      "   251020_ENT_2900__activity (7).csv: 258,656 rows\n",
      "   251020_ENT_2900__activity (1).csv: 156,881 rows\n",
      "\n",
      "üíæ SUCCESS! Combined file saved as: combined_all_data.csv\n",
      "   Location: /Users/jacobmarchand/Data Science Templates/combined_all_data.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Your directory\n",
    "csv_directory = \"/Applications/WorkDataSets/DataStore/Demandbase CDP 4.0/\"\n",
    "\n",
    "# Get all CSV files in the directory\n",
    "csv_files = glob.glob(os.path.join(csv_directory, \"*.csv\"))\n",
    "\n",
    "print(f\"üîç Found {len(csv_files)} CSV files:\")\n",
    "for file in csv_files:\n",
    "    print(f\"  - {os.path.basename(file)}\")\n",
    "\n",
    "if len(csv_files) == 0:\n",
    "    print(\"‚ùå No CSV files found in the directory!\")\n",
    "else:\n",
    "    # Combine all CSV files\n",
    "    combined_data = []\n",
    "    \n",
    "    for file in csv_files:\n",
    "        print(f\"\\nüìñ Loading: {os.path.basename(file)}\")\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        # Optional: Add source file column to track where each row came from\n",
    "        df['source_file'] = os.path.basename(file)\n",
    "        \n",
    "        combined_data.append(df)\n",
    "        print(f\"   ‚úÖ Loaded {len(df):,} rows\")\n",
    "    \n",
    "    # Concatenate all dataframes\n",
    "    final_df = pd.concat(combined_data, ignore_index=True)\n",
    "    \n",
    "    # Results summary\n",
    "    print(f\"\\nüéâ COMBINATION COMPLETE!\")\n",
    "    print(f\"   Files combined: {len(csv_files)}\")\n",
    "    print(f\"   Total rows: {len(final_df):,}\")\n",
    "    print(f\"   Total columns: {len(final_df.columns)}\")\n",
    "    \n",
    "    # Show breakdown by source file\n",
    "    print(f\"\\nüìä Rows per file:\")\n",
    "    source_counts = final_df['source_file'].value_counts()\n",
    "    for filename, count in source_counts.items():\n",
    "        print(f\"   {filename}: {count:,} rows\")\n",
    "    \n",
    "    # Save the combined file\n",
    "    output_file = \"combined_all_data.csv\"\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\nüíæ SUCCESS! Combined file saved as: {output_file}\")\n",
    "    print(f\"   Location: {os.path.abspath(output_file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0723df39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9p/ktbdntkn6p388k364f6v6n500000gn/T/ipykernel_61675/4182591563.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_df = pd.read_csv(\"/Users/jacobmarchand/Data Science Templates/combined_all_data.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1977398 entries, 0 to 1977397\n",
      "Data columns (total 19 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   Account Name           object \n",
      " 1   First Name             object \n",
      " 2   Last Name              object \n",
      " 3   Title                  object \n",
      " 4   Buying Role            object \n",
      " 5   Type                   object \n",
      " 6   Details                object \n",
      " 7   Engagement Minutes     float64\n",
      " 8   Activity Date          object \n",
      " 9   Job Title              object \n",
      " 10  Citrix Events Opt-Out  float64\n",
      " 11  Ispartner              object \n",
      " 12  Account Number         float64\n",
      " 13  Party Number           object \n",
      " 14  Territory              float64\n",
      " 15  Fatigue Level          object \n",
      " 16  Party Name             float64\n",
      " 17  CustomerId_NAR         object \n",
      " 18  source_file            object \n",
      "dtypes: float64(5), object(14)\n",
      "memory usage: 286.6+ MB\n"
     ]
    }
   ],
   "source": [
    "combined_df = pd.read_csv(\"/Users/jacobmarchand/Data Science Templates/combined_all_data.csv\")\n",
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85be48e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5386 entries, 0 to 5385\n",
      "Data columns (total 12 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   Oracle Account Customer ID                5386 non-null   object \n",
      " 1   Oracle Account Customer Name              5386 non-null   object \n",
      " 2   Oracle Account Account Segmentation       5386 non-null   object \n",
      " 3   Oracle Account Business Unit              5386 non-null   object \n",
      " 4   Eloqua Accounts Account Engagement Score  5259 non-null   float64\n",
      " 5   AE Person Name                            5386 non-null   object \n",
      " 6   AE Level14 Territory Name                 5386 non-null   object \n",
      " 7   ATS Team Person Name                      4358 non-null   object \n",
      " 8   Oracle Account Line of Business           5369 non-null   object \n",
      " 9   Oracle Account Country                    5386 non-null   object \n",
      " 10  Arr Total Arr                             5386 non-null   object \n",
      " 11  Arr Next Renewal Date                     5386 non-null   object \n",
      "dtypes: float64(1), object(11)\n",
      "memory usage: 505.1+ KB\n"
     ]
    }
   ],
   "source": [
    "AcD = pd.read_csv('/Applications/WorkDataSets/DataStore/Account Details.csv')\n",
    "AcD.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9b392db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Create normalized versions (keep only digits)\n",
    "combined_df['CustomerId_NAR_norm'] = combined_df['CustomerId_NAR'].astype(str).str.extract('(\\d+)', expand=False)\n",
    "AcD['Oracle_CustID_norm'] = AcD['Oracle Account Customer ID'].astype(str).str.extract('(\\d+)', expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee2faf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Unique matching accounts: 1689\n"
     ]
    }
   ],
   "source": [
    "# Unique values of each\n",
    "combined_unique_ids = combined_df['CustomerId_NAR_norm'].dropna().unique()\n",
    "acd_unique_ids = AcD['Oracle_CustID_norm'].dropna().unique()\n",
    "\n",
    "# Unique matches\n",
    "unique_matches = set(combined_unique_ids) & set(acd_unique_ids)\n",
    "print(f\"‚úÖ Unique matching accounts: {len(unique_matches)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b116536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total matched activity rows (non-unique): 1585076\n"
     ]
    }
   ],
   "source": [
    "# Filter activity rows that match AcD accounts\n",
    "combined_matches_df = combined_df[combined_df['CustomerId_NAR_norm'].isin(unique_matches)]\n",
    "\n",
    "# Total (non-unique) matches\n",
    "total_activity_matches = len(combined_matches_df)\n",
    "print(f\"üìä Total matched activity rows (non-unique): {total_activity_matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed7904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in AcD: 5386\n",
      "Unique Oracle Account Customer IDs: 4087\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45d1c92",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Applications/WorkDataSets/DataStore/Demandbase CDP 4.0/DemandbaseDataCDP'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m database = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/Applications/WorkDataSets/DataStore/Demandbase CDP 4.0/DemandbaseDataCDP\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m database.info()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/jupyter_env/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/jupyter_env/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/jupyter_env/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/jupyter_env/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/jupyter_env/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m    876\u001b[39m             encoding=ioargs.encoding,\n\u001b[32m    877\u001b[39m             errors=errors,\n\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Applications/WorkDataSets/DataStore/Demandbase CDP 4.0/DemandbaseDataCDP'"
     ]
    }
   ],
   "source": [
    "database = pd.read_csv('/Applications/WorkDataSets/DataStore/Demandbase CDP 4.0/DemandbaseDataCDP')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3d386ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Unique party numbers: 770\n",
      "üìä Total rows: 14235\n",
      "üîÅ Duplicate party numbers: 13465\n"
     ]
    }
   ],
   "source": [
    "unique_party_numbers = database['party_number'].nunique()\n",
    "total_rows = len(database)\n",
    "\n",
    "print(f\"‚úÖ Unique party numbers: {unique_party_numbers}\")\n",
    "print(f\"üìä Total rows: {total_rows}\")\n",
    "print(f\"üîÅ Duplicate party numbers: {total_rows - unique_party_numbers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aee80bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only digits from ID fields\n",
    "combined_df['CustomerId_NAR_norm'] = combined_df['CustomerId_NAR'].astype(str).str.extract('(\\d+)', expand=False)\n",
    "database['party_number_norm'] = database['party_number'].astype(str).str.extract('(\\d+)', expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8bbf30b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid account IDs to match against: 770\n"
     ]
    }
   ],
   "source": [
    "valid_party_ids = database['party_number_norm'].dropna().unique()\n",
    "print(f\"Number of valid account IDs to match against: {len(valid_party_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a621b996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b9640b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_activity_df = combined_df[combined_df['CustomerId_NAR_norm'].isin(valid_party_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd6d4707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original combined_df rows: 1977398\n",
      "Filtered rows (only matched accounts): 477720\n",
      "Unique accounts in filtered activity: 450\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original combined_df rows: {len(combined_df)}\")\n",
    "print(f\"Filtered rows (only matched accounts): {len(filtered_activity_df)}\")\n",
    "print(f\"Unique accounts in filtered activity: {filtered_activity_df['CustomerId_NAR_norm'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8641c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_activity_df.to_csv('/Applications/WorkDataSets/DataStore/Demandbase CDP 4.0/DemandbaseDataCDP2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter_env)",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
